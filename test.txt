# pip install lightgbm scikit-learn joblib pandas numpy scipy
import numpy as np
import pandas as pd
from typing import Optional, List, Dict
from joblib import Parallel, delayed
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score
from lightgbm import LGBMClassifier
from scipy.stats import randint as sp_randint, uniform as sp_uniform, loguniform as sp_loguniform, rv_discrete

# ---- Param distributions as scipy.stats distributions ----
#_max_depth_vals = np.array([-1, 3, 4, 5, 6, 7, 8])
_max_depth_probs = np.full(len(_max_depth_vals), 1/len(_max_depth_vals))
MAX_DEPTH_DIST = rv_discrete(name="max_depth", values=(_max_depth_vals, _max_depth_probs))

PARAM_DIST = {
    "clf__num_leaves": sp_randint(3, 5),
    "clf__min_child_samples": sp_randint(10, 200),
   # "clf__max_depth": MAX_DEPTH_DIST,
    "clf__learning_rate": sp_loguniform(1e-2, 2e-1),
    "clf__subsample": sp_uniform(loc=0.7, scale=0.3),
    "clf__colsample_bytree": sp_uniform(loc=0.7, scale=0.3),
    "clf__reg_lambda": sp_loguniform(1e-4, 10.0),
}

def _make_imputer(impute_strategy: Optional[str]) -> Optional[SimpleImputer]:
    if impute_strategy is None:
        return None
    if isinstance(impute_strategy, (int, float)) and impute_strategy == 0:
        return SimpleImputer(strategy="constant", fill_value=0)
    if str(impute_strategy).lower() in {"zero", "constant_zero"}:
        return SimpleImputer(strategy="constant", fill_value=0)
    return SimpleImputer(strategy=str(impute_strategy))

def _run_one_feature_randomcv(
    X_col: pd.Series,
    y_arr: np.ndarray,
    feature: str,
    n_iter: int,
    cv,
    seed: int,
    impute_strategy: Optional[str],
) -> Dict:
    steps = []
    imputer = _make_imputer(impute_strategy)
    if imputer is not None:
        steps.append(("imputer", imputer))

    clf = LGBMClassifier(
        objective="binary",
        metric="auc",
        n_estimators=int(2000),
        random_state=int(seed),
        n_jobs=1,
    )
    pipe = Pipeline(steps + [("clf", clf)])

    # Random search
    search = RandomizedSearchCV(
        estimator=pipe,
        param_distributions=PARAM_DIST,
        n_iter=int(n_iter),
        scoring="roc_auc",
        cv=cv,
        random_state=int(seed),
        n_jobs=1,
        refit=True,
        verbose=0,
    )
    X_feat = X_col.to_numpy().reshape(-1, 1)
    search.fit(X_feat, y_arr)

    best_cv_auc = float(search.best_score_)
    best_model = search.best_estimator_
    train_proba = best_model.predict_proba(X_feat)[:, 1]
    train_auc = float(roc_auc_score(y_arr, train_proba))

    # ---- NEW: average fold train/valid AUC using best params ----
    fold_train_aucs, fold_valid_aucs = [], []
    for tr_idx, va_idx in cv.split(X_feat, y_arr):
        X_tr, X_va = X_feat[tr_idx], X_feat[va_idx]
        y_tr, y_va = y_arr[tr_idx], y_arr[va_idx]

        m = Pipeline(steps + [("clf", LGBMClassifier(
            **{k.replace("clf__", ""): v for k, v in search.best_params_.items()},
            objective="binary",
            metric="auc",
            n_estimators=int(2000),
            random_state=int(seed),
            n_jobs=1,
        ))])
        m.fit(X_tr, y_tr)
        proba_tr = m.predict_proba(X_tr)[:, 1]
        proba_va = m.predict_proba(X_va)[:, 1]
        fold_train_aucs.append(roc_auc_score(y_tr, proba_tr))
        fold_valid_aucs.append(roc_auc_score(y_va, proba_va))

    avg_train_auc = float(np.mean(fold_train_aucs))
    avg_valid_auc = float(np.mean(fold_valid_aucs))

    prefix = "with_impute" if imputer is not None else "no_impute"
    return {
        "feature": feature,
        f"AUC_valid_{prefix}": best_cv_auc,
        f"AUC_train_{prefix}": train_auc,
        f"Avg_fold_AUC_valid_{prefix}": avg_valid_auc,
        f"Avg_fold_AUC_train_{prefix}": avg_train_auc,
        f"best_params_{prefix}": search.best_params_,
    }

def univariate_lgbm_importance_randomcv_parallel(
    df: pd.DataFrame,
    y: pd.Series,
    features: Optional[List[str]] = None,
    n_splits: int = 5,
    n_iter: int = 20,
    seed: int = 42,
    n_jobs: int = -1,
    compare_impute: bool = True,
    impute_strategy: str = "median",
) -> pd.DataFrame:
    if features is None:
        features = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]

    X = df[features]
    y_arr = y.astype(int).to_numpy()
    cv = StratifiedKFold(n_splits=int(n_splits), shuffle=True, random_state=int(seed))

    res_noimp = Parallel(n_jobs=n_jobs, prefer="processes")(
        delayed(_run_one_feature_randomcv)(
            X[col], y_arr, col, n_iter, cv, seed, impute_strategy=None
        )
        for col in features
    )
    df_noimp = pd.DataFrame(res_noimp)

    if compare_impute:
        res_imp = Parallel(n_jobs=n_jobs, prefer="processes")(
            delayed(_run_one_feature_randomcv)(
                X[col], y_arr, col, n_iter, cv, seed, impute_strategy=impute_strategy
            )
            for col in features
        )
        df_imp = pd.DataFrame(res_imp)
        out = df_noimp.merge(df_imp, on="feature", how="left")
        sort_col = "AUC_valid_with_impute" if "AUC_valid_with_impute" in out else "AUC_valid_no_impute"
    else:
        out = df_noimp
        sort_col = "AUC_valid_no_impute"

    return out.sort_values(sort_col, ascending=False).reset_index(drop=True)


from pyspark.sql import functions as F, Window as W

# --- Load base table
df = spark.table("tracracc.app_data_20250820")

# --- Flags (edit any rule logic as needed)
df = (df
  .withColumn("f01_has_acct",        (F.col("SCOPE_HAS_ACNT_FLG") == 1))
  .withColumn("f02_cust_type",       (F.col("SCOPE_CUST_TYPE_FLG") == 1))
  .withColumn("f03_mom_segm",        (F.col("SCOPE_MOM_SEGM_FLG") == 1))
  .withColumn("f04_exc_rating",      (F.coalesce(F.col("SCOPE_EXC_RATING_NBR_FLG"), F.lit(0)) == 1))
  .withColumn("f05_legal_form",      (F.col("SCOPE_LEGAL_FORM_FLG") == 1))
  .withColumn("f06_limit_amt",       (F.col("SCOPE_LMT_AMT_FLG") == 1))
  .withColumn("f07_model_prefix",    (F.col("SCOPE_MODEL_PREFIX_FLG") == 1))
  .withColumn("f08_dpd30_ok",        (F.coalesce(F.col("MAX_DPD_OVER_30_FLG"), F.lit(0)) == 1))
  .withColumn("f09_not_kos_imputed", ~((F.col("SOURCE") == F.lit("KOS")) & (F.col("IMPUTED_START_DT_FLG") == 1)))
  .withColumn("f10_max_days",        (F.col("SCOPE_MAX_DAYS_UNTIL_DISB_FLG") == 1))
  .withColumn("f11_l00_after_ok",    (F.col("L00_AFTER") >= F.lit(-50000000)))  # adjust bound if needed
  .withColumn("f12_bad_elig_ok",     (F.coalesce(F.col("MAX_BAD_ELG_UNTIL_DISB_JCO"), F.lit(0)) == 1))
  .withColumn("f13_main_grid_ok",    (F.col("MAIN_GRID_ID").isNotNull()))
  .withColumn("f14_bad_flag_notnull",(F.col("MAX_BAD_FLG_24M_JCO").isNotNull()))
  .withColumn("f15_ctry_ok",         (F.coalesce(F.col("CTRY_OF_RESIDENCE_MAIN"), F.lit("NL")).isin("NL","ENL")))
)

# --- Cumulative passes
dfp = (df
  .withColumn("p01", F.col("f01_has_acct"))
  .withColumn("p02", F.col("p01") & F.col("f02_cust_type"))
  .withColumn("p03", F.col("p02") & F.col("f03_mom_segm"))
  .withColumn("p04", F.col("p03") & F.col("f04_exc_rating"))
  .withColumn("p05", F.col("p04") & F.col("f05_legal_form"))
  .withColumn("p06", F.col("p05") & F.col("f06_limit_amt"))
  .withColumn("p07", F.col("p06") & F.col("f07_model_prefix"))
  .withColumn("p08", F.col("p07") & F.col("f08_dpd30_ok"))
  .withColumn("p09", F.col("p08") & F.col("f09_not_kos_imputed"))
  .withColumn("p10", F.col("p09") & F.col("f10_max_days"))
  .withColumn("p11", F.col("p10") & F.col("f11_l00_after_ok"))
  .withColumn("p12", F.col("p11") & F.col("f12_bad_elig_ok"))
  .withColumn("p13", F.col("p12") & F.col("f13_main_grid_ok"))
  .withColumn("p14", F.col("p13") & F.col("f14_bad_flag_notnull"))
  .withColumn("p15", F.col("p14") & F.col("f15_ctry_ok"))
)

# --- Helper: distinct count of prod_key conditional on an expression
def cnt(expr):
    return F.countDistinct(F.when(expr, F.col("prod_key")))

# --- Aggregate (remaining after each step + bads remaining after each step)
wide_counts = dfp.agg(
    # Remaining
    F.countDistinct("prod_key").alias("after_00_universe"),
    cnt(F.col("p01")).alias("after_01_has_acct"),
    cnt(F.col("p02")).alias("after_02_cust_type"),
    cnt(F.col("p03")).alias("after_03_mom_segm"),
    cnt(F.col("p04")).alias("after_04_exc_rating"),
    cnt(F.col("p05")).alias("after_05_legal_form"),
    cnt(F.col("p06")).alias("after_06_limit_amt"),
    cnt(F.col("p07")).alias("after_07_model_prefix"),
    cnt(F.col("p08")).alias("after_08_dpd30_ok"),
    cnt(F.col("p09")).alias("after_09_not_kos_imputed"),
    cnt(F.col("p10")).alias("after_10_max_days"),
    cnt(F.col("p11")).alias("after_11_l00_after_ok"),
    cnt(F.col("p12")).alias("after_12_bad_elig_ok"),
    cnt(F.col("p13")).alias("after_13_main_grid_ok"),
    cnt(F.col("p14")).alias("after_14_bad_flag_notnull"),
    cnt(F.col("p15")).alias("after_15_ctry_ok"),

    # Bads remaining (MAX_BAD_FLG_24M_JCO = 1) at each stage
    cnt(F.col("MAX_BAD_FLG_24M_JCO") == 1).alias("bads_00_universe"),
    cnt(F.col("p01") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_01_has_acct"),
    cnt(F.col("p02") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_02_cust_type"),
    cnt(F.col("p03") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_03_mom_segm"),
    cnt(F.col("p04") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_04_exc_rating"),
    cnt(F.col("p05") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_05_legal_form"),
    cnt(F.col("p06") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_06_limit_amt"),
    cnt(F.col("p07") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_07_model_prefix"),
    cnt(F.col("p08") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_08_dpd30_ok"),
    cnt(F.col("p09") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_09_not_kos_imputed"),
    cnt(F.col("p10") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_10_max_days"),
    cnt(F.col("p11") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_11_l00_after_ok"),
    cnt(F.col("p12") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_12_bad_elig_ok"),
    cnt(F.col("p13") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_13_main_grid_ok"),
    cnt(F.col("p14") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_14_bad_flag_notnull"),
    cnt(F.col("p15") & (F.col("MAX_BAD_FLG_24M_JCO") == 1)).alias("bads_15_ctry_ok")
)

# --- Unpivot to long (one row per step)
labels = [
    "Universe",
    "Has account",
    "Customer type",
    "MOM segment",
    "Excl rating OK",
    "Legal form",
    "Limit amount",
    "Model prefix",
    "DPD>30 rule",
    "Not KOS+imputed",
    "Max days until disb",
    "L00 after bound",
    "Bad eligibility",
    "Main grid present",
    "Bad flag not null",
    "Country check",
]

# Build arrays for remaining and bads from the single wide_counts row
long_df = (wide_counts
  .select(
    F.array(
      F.col("after_00_universe"),
      F.col("after_01_has_acct"),
      F.col("after_02_cust_type"),
      F.col("after_03_mom_segm"),
      F.col("after_04_exc_rating"),
      F.col("after_05_legal_form"),
      F.col("after_06_limit_amt"),
      F.col("after_07_model_prefix"),
      F.col("after_08_dpd30_ok"),
      F.col("after_09_not_kos_imputed"),
      F.col("after_10_max_days"),
      F.col("after_11_l00_after_ok"),
      F.col("after_12_bad_elig_ok"),
      F.col("after_13_main_grid_ok"),
      F.col("after_14_bad_flag_notnull"),
      F.col("after_15_ctry_ok"),
    ).alias("remaining_arr"),
    F.array(
      F.col("bads_00_universe"),
      F.col("bads_01_has_acct"),
      F.col("bads_02_cust_type"),
      F.col("bads_03_mom_segm"),
      F.col("bads_04_exc_rating"),
      F.col("bads_05_legal_form"),
      F.col("bads_06_limit_amt"),
      F.col("bads_07_model_prefix"),
      F.col("bads_08_dpd30_ok"),
      F.col("bads_09_not_kos_imputed"),
      F.col("bads_10_max_days"),
      F.col("bads_11_l00_after_ok"),
      F.col("bads_12_bad_elig_ok"),
      F.col("bads_13_main_grid_ok"),
      F.col("bads_14_bad_flag_notnull"),
      F.col("bads_15_ctry_ok"),
    ).alias("bads_arr")
  )
  .select(
    F.posexplode("remaining_arr").alias("step","remaining"),
    F.col("bads_arr")[F.col("step")].alias("bads_remaining")
  )
)

# Add labels, drop count, and bad rate
w = W.orderBy("step")
waterfall_df = (long_df
  .withColumn("label", F.array([F.lit(x) for x in labels])[F.col("step")])
  .withColumn("prev_remaining", F.lag("remaining").over(w))
  .withColumn("dropped_at_step", F.when(F.col("prev_remaining").isNull(), F.lit(0))
                                  .otherwise(F.col("prev_remaining") - F.col("remaining")))
  .withColumn("bad_rate", F.when(F.col("remaining") > 0, F.col("bads_remaining") / F.col("remaining")))
  .select("step","label","remaining","dropped_at_step","bads_remaining","bad_rate")
  .orderBy("step")
)

# --- Use it
# display(waterfall_df)            # Databricks
# waterfall_df.show(50, truncate=False)
# waterfall_df.write.mode("overwrite").format("delta").saveAsTable("sandbox.waterfall_app_filters")



