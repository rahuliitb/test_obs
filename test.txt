strict_binning_params = {
    "variable_names": None,            # List of features to bin. MUST be set before initializing BinningProcess.

    "max_n_prebins": 10,               # Max pre-bins before optimization. Lower = smoother, less overfitting.
    "min_prebin_size": 0.08,           # Each pre-bin must contain ≥ 8% of samples → stable grouping.

    "min_n_bins": 2,                   # Final bins must be at least 2 (prevents trivial single-bin cases).
    "max_n_bins": 5,                   # Caps total bins → prevents overly granular splits that overfit.

    "min_bin_size": 0.05,              # Each final bin must contain ≥ 5% samples → avoids tiny, unstable bins.
    "max_bin_size": 0.5,               # Upper bound on bin size → prevents overly dominant bins.

    "max_pvalue": 0.05,                # Adjacent bins must differ statistically (p ≤ 0.05). Merges weak splits.
    "max_pvalue_policy": "consecutive",# Only compare consecutive bins for p-value test; stable & efficient.

    # Variable selection to avoid weak predictors.
    "selection_criteria": {
        "iv": {                        # Information Value (IV) filter:
            "min": 0.02,               # Keep only variables with IV ≥ 0.02 (weak but meaningful).
            "strategy": "highest",     # Rank variables by highest IV.
            "top": 0.40                # Keep top 40% IV variables → strong dimensionality reduction.
        },
        "ks": {                        # Kolmogorov–Smirnov statistic filter:
            "min": 0.05                # KS ≥ 0.05 ensures separation between good/bad distributions.
        }
    },

    "fixed_variables": None,           # Variables that must be kept regardless of selection_criteria.

    "categorical_variables": None,     # Variables treated as categorical (numeric but nominal). Used in binning.

    "special_codes": None,             # Values treated separately (e.g., -999 for missing codes).

    "split_digits": 3,                 # Round final split points to 3 significant digits → stable & interpretable.

    # Per-variable overrides:
    "binning_fit_params": None,        # Dict: {"var1": {"max_n_bins": 4, ...}} custom fit rules per variable.
    "binning_transform_params": None,  # Dict: {"var1": {"metric": "event_rate"}} custom transform per variable.

    "n_jobs": 1,                       # Single-threaded deterministic execution (recommended for reproducibility).
    "verbose": False                   # Set True to inspect binning decisions and merging behavior.
}




# Build pipeline for selected features
    # ------------------------
    def _build_pipeline(self):
        if self.selected_features_ is None:
            raise RuntimeError("Call select_features() before building pipeline.")

        binning = BinningProcess(
            variable_names=self.selected_features_,
            special_codes=self.special_codes,
            binning_fit_params=self.binning_fit_params
        )
        scaler = StandardScaler()

        # Choose solver based on regularization options
        if self.regularization == "both":
            # saga supports l1 and l2 (and is robust for large sparse data)
            solver = "saga"
        else:
            # default solver for l2
            solver = "lbfgs"

        clf = LogisticRegression(
            max_iter=2000,
            random_state=self.random_state,
            solver=solver
        )

        pipe = Pipeline([("binning", binning), ("scaler", scaler), ("clf", clf)])
        return pipe

    # ------------------------
    # Construct default param grid (adapts to regularization choice)
    # ------------------------
    def _build_param_grid(self) -> Dict[str, Any]:
        if self._user_param_grid is not None:
            return self._user_param_grid

        # base grid: tune C (inverse regularization strength)
        base_grid = {
            "clf__C": [0.001, 0.01, 0.1, 1.0, 10.0]
        }

        if self.regularization == "both":
            # allow both penalties
            base_grid["clf__penalty"] = ["l1", "l2"]
            # saga supports penalty l1 and l2; if using saga, solver ok
            # optionally keep class_weight tuning
            base_grid["clf__class_weight"] = [None, "balanced"]
        else:
            # only l2: no penalty hyperparameter needed (solver default 'lbfgs' supports l2)
            base_grid["clf__class_weight"] = [None, "balanced"]

        return base_grid
