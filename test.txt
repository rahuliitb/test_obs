from typing import List, Dict, Any, Optional
import numpy as np
import pandas as pd
from optbinning import BinningProcess
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import roc_auc_score
from scipy.stats import loguniform


class ChallengerModel:
    """
    Credit-risk challenger model using OptBinning, IV-based feature selection,
    logistic regression, randomized hyperparameter tuning, grouped CV, and
    out-of-time (OOT) evaluation.
    """

    def __init__(
        self,
        numeric_cols: List[str],
        iv_threshold: float = 0.02,
        min_features: int = 1,
        max_features: int = 15,
        scoring: str = "roc_auc",
        binning_params: Optional[Dict[str, Any]] = None,
        default_clf_params: Optional[Dict[str, Any]] = None,
        n_iter: int = 20,
        n_jobs: int = -1,
        verbose: int = 0,
    ) -> None:
        """
        Initialize the ChallengerModel.

        Args:
            numeric_cols: Input numerical feature names.
            iv_threshold: Minimum Information Value threshold.
            min_features: Minimum number of features required.
            max_features: Maximum number of features allowed.
            scoring: Metric used during model evaluation.
            binning_params: Parameters forwarded to OptBinning.
            default_clf_params: Default LogisticRegression parameters.
            n_iter: Number of hyperparameter search iterations.
            n_jobs: Number of parallel jobs.
            verbose: Verbosity level.
        """
        self.numeric_cols = numeric_cols
        self.iv_threshold = iv_threshold
        self.min_features = min_features
        self.max_features = max_features
        self.scoring = scoring

        self.binning_params = binning_params or {}
        self.default_clf_params = default_clf_params or {
            "penalty": "l2",
            "solver": "lbfgs",
            "C": 1.0,
            "max_iter": 2000,
            "class_weight": None,
        }

        self.n_iter = n_iter
        self.n_jobs = n_jobs
        self.verbose = verbose

        self.selected_features_: Optional[List[str]] = None
        self.iv_report_: Optional[pd.DataFrame] = None
        self.final_model_: Optional[Pipeline] = None
        self.best_params_: Optional[Dict[str, Any]] = None
        self.best_cv_score_: Optional[float] = None
        self.it_auc: Optional[float] = None
        self.oot_auc: Optional[float] = None

    def _compute_feature_report(
        self,
        X_train: pd.DataFrame,
        y_train: np.ndarray,
    ) -> None:
        """
        Fit BinningProcess, obtain full summary, and select optimal features.

        Args:
            X_train: Training input features.
            y_train: Training target labels.

        Raises:
            ValueError: If fewer than `min_features` are selected.
        """
        bp = BinningProcess(
            variable_names=self.numeric_cols,
            **self.binning_params,
        )
        bp.fit(X_train, y_train)

        summary = bp.summary()

        # Keep only selected (optimal) variables
        summary_opt = summary[summary["selected"]]

        selected = summary_opt["variable"].tolist()

        if len(selected) < self.min_features:
            raise ValueError(
                f"Only {len(selected)} features selected, "
                f"minimum required is {self.min_features}."
            )

        # Apply max-features constraint
        if len(selected) > self.max_features:
            sorted_iv = (
                summary_opt.sort_values("iv", ascending=False)["variable"].tolist()
            )
            selected = sorted_iv[: self.max_features]

        self.iv_report_ = summary_opt
        self.selected_features_ = selected

    def _build_pipeline(self, tuning: bool) -> Pipeline:
        """
        Build the modeling pipeline: OptBinning → StandardScaler → LogisticRegression.

        Args:
            tuning: Whether pipeline is used for hyperparameter tuning.

        Returns:
            Pipeline: sklearn modeling pipeline.
        """
        if tuning:
            clf = LogisticRegression(solver="saga", max_iter=5000)
        else:
            clf = LogisticRegression(**self.default_clf_params)

        return Pipeline(
            [
                (
                    "binning",
                    BinningProcess(
                        variable_names=self.selected_features_,
                        **self.binning_params,
                    ),
                ),
                ("scaler", StandardScaler()),
                ("clf", clf),
            ]
        )

    def fit(
        self,
        X_train: pd.DataFrame,
        y_train: np.ndarray,
        X_oot: pd.DataFrame,
        y_oot: np.ndarray,
        cv,
        groups_train: np.ndarray,
        tune_hyperparameters: bool = True,
        param_distributions: Optional[Dict[str, Any]] = None,
    ) -> "ChallengerModel":
        """
        Fit model using training data, with optional hyperparameter tuning.

        Args:
            X_train: Training feature matrix.
            y_train: Training target labels.
            X_oot: Out-of-time feature matrix.
            y_oot: Out-of-time target labels.
            cv: Group-aware cross-validation splitter.
            groups_train: Group labels for each training row.
            tune_hyperparameters: Whether to run RandomizedSearchCV.
            param_distributions: Search space for hyperparameter tuning.

        Returns:
            ChallengerModel: The fitted model instance.
        """
        self._compute_feature_report(X_train, y_train)

        X = X_train[self.selected_features_]
        Xo = X_oot[self.selected_features_]

        pipe = self._build_pipeline(tune_hyperparameters)

        if tune_hyperparameters:
            if param_distributions is None:
                param_distributions = {
                    "clf__C": loguniform(1e-4, 1e1),
                    "clf__class_weight": [None, "balanced"],
                    "clf__penalty": ["l1", "l2"],
                }

            rs = RandomizedSearchCV(
                estimator=pipe,
                param_distributions=param_distributions,
                n_iter=self.n_iter,
                scoring=self.scoring,
                cv=cv,
                groups=groups_train,
                refit=True,
                n_jobs=self.n_jobs,
                verbose=self.verbose,
                random_state=42,
            )
            rs.fit(X, y_train)
            self.final_model_ = rs.best_estimator_
            self.best_params_ = rs.best_params_
            self.best_cv_score_ = rs.best_score_
        else:
            cv_scores = cross_val_score(
                pipe,
                X,
                y_train,
                cv=cv,
                groups=groups_train,
                scoring=self.scoring,
                n_jobs=self.n_jobs,
            )
            self.best_cv_score_ = float(np.mean(cv_scores))

            pipe.fit(X, y_train)
            self.final_model_ = pipe
            self.best_params_ = pipe.named_steps["clf"].get_params()

        # Compute AUCs
        self.it_auc = roc_auc_score(y_train, self.final_model_.predict_proba(X)[:, 1])
        self.oot_auc = roc_auc_score(y_oot, self.final_model_.predict_proba(Xo)[:, 1])

        return self

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict positive-class probabilities.

        Args:
            X: Input dataframe.

        Returns:
            np.ndarray: Probability scores.
        """
        return self.final_model_.predict_proba(
            X[self.selected_features_]
        )[:, 1]

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict binary class labels.

        Args:
            X: Input dataframe.

        Returns:
            np.ndarray: Predicted class labels.
        """
        return self.final_model_.predict(X[self.selected_features_])
