# pip install lightgbm scikit-learn joblib pandas numpy scipy
import numpy as np
import pandas as pd
from typing import Optional, List, Dict
from joblib import Parallel, delayed
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score
from lightgbm import LGBMClassifier
from scipy.stats import randint as sp_randint, uniform as sp_uniform, loguniform as sp_loguniform, rv_discrete

# ---- Param distributions as scipy.stats distributions ----
#_max_depth_vals = np.array([-1, 3, 4, 5, 6, 7, 8])
_max_depth_probs = np.full(len(_max_depth_vals), 1/len(_max_depth_vals))
MAX_DEPTH_DIST = rv_discrete(name="max_depth", values=(_max_depth_vals, _max_depth_probs))

PARAM_DIST = {
    "clf__num_leaves": sp_randint(3, 5),
    "clf__min_child_samples": sp_randint(10, 200),
   # "clf__max_depth": MAX_DEPTH_DIST,
    "clf__learning_rate": sp_loguniform(1e-2, 2e-1),
    "clf__subsample": sp_uniform(loc=0.7, scale=0.3),
    "clf__colsample_bytree": sp_uniform(loc=0.7, scale=0.3),
    "clf__reg_lambda": sp_loguniform(1e-4, 10.0),
}

def _make_imputer(impute_strategy: Optional[str]) -> Optional[SimpleImputer]:
    if impute_strategy is None:
        return None
    if isinstance(impute_strategy, (int, float)) and impute_strategy == 0:
        return SimpleImputer(strategy="constant", fill_value=0)
    if str(impute_strategy).lower() in {"zero", "constant_zero"}:
        return SimpleImputer(strategy="constant", fill_value=0)
    return SimpleImputer(strategy=str(impute_strategy))

def _run_one_feature_randomcv(
    X_col: pd.Series,
    y_arr: np.ndarray,
    feature: str,
    n_iter: int,
    cv,
    seed: int,
    impute_strategy: Optional[str],
) -> Dict:
    steps = []
    imputer = _make_imputer(impute_strategy)
    if imputer is not None:
        steps.append(("imputer", imputer))

    clf = LGBMClassifier(
        objective="binary",
        metric="auc",
        n_estimators=int(2000),
        random_state=int(seed),
        n_jobs=1,
    )
    pipe = Pipeline(steps + [("clf", clf)])

    # Random search
    search = RandomizedSearchCV(
        estimator=pipe,
        param_distributions=PARAM_DIST,
        n_iter=int(n_iter),
        scoring="roc_auc",
        cv=cv,
        random_state=int(seed),
        n_jobs=1,
        refit=True,
        verbose=0,
    )
    X_feat = X_col.to_numpy().reshape(-1, 1)
    search.fit(X_feat, y_arr)

    best_cv_auc = float(search.best_score_)
    best_model = search.best_estimator_
    train_proba = best_model.predict_proba(X_feat)[:, 1]
    train_auc = float(roc_auc_score(y_arr, train_proba))

    # ---- NEW: average fold train/valid AUC using best params ----
    fold_train_aucs, fold_valid_aucs = [], []
    for tr_idx, va_idx in cv.split(X_feat, y_arr):
        X_tr, X_va = X_feat[tr_idx], X_feat[va_idx]
        y_tr, y_va = y_arr[tr_idx], y_arr[va_idx]

        m = Pipeline(steps + [("clf", LGBMClassifier(
            **{k.replace("clf__", ""): v for k, v in search.best_params_.items()},
            objective="binary",
            metric="auc",
            n_estimators=int(2000),
            random_state=int(seed),
            n_jobs=1,
        ))])
        m.fit(X_tr, y_tr)
        proba_tr = m.predict_proba(X_tr)[:, 1]
        proba_va = m.predict_proba(X_va)[:, 1]
        fold_train_aucs.append(roc_auc_score(y_tr, proba_tr))
        fold_valid_aucs.append(roc_auc_score(y_va, proba_va))

    avg_train_auc = float(np.mean(fold_train_aucs))
    avg_valid_auc = float(np.mean(fold_valid_aucs))

    prefix = "with_impute" if imputer is not None else "no_impute"
    return {
        "feature": feature,
        f"AUC_valid_{prefix}": best_cv_auc,
        f"AUC_train_{prefix}": train_auc,
        f"Avg_fold_AUC_valid_{prefix}": avg_valid_auc,
        f"Avg_fold_AUC_train_{prefix}": avg_train_auc,
        f"best_params_{prefix}": search.best_params_,
    }

def univariate_lgbm_importance_randomcv_parallel(
    df: pd.DataFrame,
    y: pd.Series,
    features: Optional[List[str]] = None,
    n_splits: int = 5,
    n_iter: int = 20,
    seed: int = 42,
    n_jobs: int = -1,
    compare_impute: bool = True,
    impute_strategy: str = "median",
) -> pd.DataFrame:
    if features is None:
        features = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]

    X = df[features]
    y_arr = y.astype(int).to_numpy()
    cv = StratifiedKFold(n_splits=int(n_splits), shuffle=True, random_state=int(seed))

    res_noimp = Parallel(n_jobs=n_jobs, prefer="processes")(
        delayed(_run_one_feature_randomcv)(
            X[col], y_arr, col, n_iter, cv, seed, impute_strategy=None
        )
        for col in features
    )
    df_noimp = pd.DataFrame(res_noimp)

    if compare_impute:
        res_imp = Parallel(n_jobs=n_jobs, prefer="processes")(
            delayed(_run_one_feature_randomcv)(
                X[col], y_arr, col, n_iter, cv, seed, impute_strategy=impute_strategy
            )
            for col in features
        )
        df_imp = pd.DataFrame(res_imp)
        out = df_noimp.merge(df_imp, on="feature", how="left")
        sort_col = "AUC_valid_with_impute" if "AUC_valid_with_impute" in out else "AUC_valid_no_impute"
    else:
        out = df_noimp
        sort_col = "AUC_valid_no_impute"

    return out.sort_values(sort_col, ascending=False).reset_index(drop=True)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GroupKFold
from feature_engine.selection import DropHighPSIFeatures

# # ---------- Simulate data: 10k customers per year, 2019 vs 2020 with drift ----------
# rng = np.random.default_rng(123)
# n_per_year = 10_000

# months = np.array([f"{m:02d}" for m in range(1, 13)])

# def make_year(year, n, mean, sd=10):
#     mlabels = np.tile(months, int(np.ceil(n/12)))[:n]
#     rng.shuffle(mlabels)
#     yyyy_mm = [f"{year}-{m}" for m in mlabels]
#     return pd.DataFrame({
#         "YYYYMM": yyyy_mm,
#         "year": year,
#         "feat1": rng.normal(mean, sd, size=n)
#     })

# df = pd.concat([
#     make_year(2019, n_per_year, mean=50, sd=10),
#     make_year(2020, n_per_year, mean=60, sd=10),
#     #make_year(2021, n_per_year, mean=60, sd=10)
# ], ignore_index=True)
import pandas as pd
import numpy as np

# ---------- Simulate data: 10k customers per year, 2019 vs 2020 with drift ----------
rng = np.random.default_rng(123)
n_per_year = 10_000

months = np.array([f"{m:02d}" for m in range(1, 13)])

def make_year(year, n, mean, sd=10):
    mlabels = np.tile(months, int(np.ceil(n/12)))[:n]
    rng.shuffle(mlabels)
    yyyy_mm = [f"{year}-{m}" for m in mlabels]
    df = pd.DataFrame({
        "YYYYMM": yyyy_mm,
        "year": year,
        "feat1": rng.normal(mean, sd, size=n)
    })
    
    # Add proper datetime column (set day=01 for simplicity)
    df["date"] = pd.to_datetime(df["YYYYMM"] + "-01")
    
    # Add period-based columns
    df["month"] = df["date"].dt.month
    df["quarter"] = df["date"].dt.to_period("Q").astype(str)   # e.g., '2019Q1'
    df["half_year"] = np.where(df["date"].dt.quarter <= 2, "H1", "H2")
    df["year_half"] = df["year"].astype(str) + df["half_year"]  # e.g., '2019H1'
    
    return df

df = pd.concat([
    make_year(2019, n_per_year, mean=50, sd=10),
    make_year(2020, n_per_year, mean=60, sd=10),

], ignore_index=True)



# # Simulate initial full dataset
# rng = np.random.default_rng(2025)

# def simulate_year(year, n_total, base_mean, base_sd=10.0,
#                   seasonal_amp=2.0, noise_sd=0.8):
#     months = np.arange(1, 13)
#     counts = np.repeat(n_total // 12, 12)
#     seasonal = seasonal_amp * np.sin(2 * np.pi * (months - 1) / 12.0)
#     month_means = base_mean + seasonal + rng.normal(0.0, noise_sd, size=12)
#     rows = []
#     for m, cnt in zip(months, counts):
#         vals = rng.normal(loc=month_means[m-1], scale=base_sd, size=cnt)
#         rows.append(pd.DataFrame({"year": year, "YYYYMM": f"{year}-{m:02d}", "feat1": vals}))
#     return pd.concat(rows, ignore_index=True)

# # Start with equal monthly counts, then drop randomly to make uneven
# N_PER_YEAR = 12_000
# df2019 = simulate_year(2019, N_PER_YEAR, base_mean=50.0, base_sd=10.0)
# df2020 = simulate_year(2020, N_PER_YEAR, base_mean=55.0, base_sd=10.0)

# def drop_randomly(df, min_frac=0.5, max_frac=0.9):
#     out = []
#     for month in df["YYYYMM"].unique():
#         sub = df[df["YYYYMM"] == month]
#         frac = rng.uniform(min_frac, max_frac)
#         out.append(sub.sample(frac=frac, random_state=2025))
#     return pd.concat(out, ignore_index=True)

# df2019_uneven = drop_randomly(df2019, min_frac=0.5, max_frac=0.9)
# df2020_uneven = drop_randomly(df2020, min_frac=0.5, max_frac=0.9)

# df = pd.concat([df2019_uneven, df2020_uneven], ignore_index=True)



# ---------- Pure PSI: 2019 (basis) vs 2020 (test) in both directions ----------
df_pure = df.copy()
df_pure["split"] = np.where(df_pure["year"] == 2019, "basis", "test")

dhp_fwd = DropHighPSIFeatures(split_col="split", bins=10, strategy="equal_frequency")
dhp_fwd.fit(df_pure[["feat1", "split"]])
psi_pure_forward = dhp_fwd.psi_values_["feat1"]

dhp_rev = DropHighPSIFeatures(split_col="split", bins=10, strategy="equal_frequency", switch=True)
dhp_rev.fit(df_pure[["feat1", "split"]])
psi_pure_reverse = dhp_rev.psi_values_["feat1"]

# ---------- GroupKFold(5) on YYYYMM with forward & reverse per fold ----------
gkf = GroupKFold(n_splits=5, shuffle=True, random_state=23)
rows = []

group = "month"
for fold, (train_idx, test_idx) in enumerate(gkf.split(df, groups=df[group])):
    tmp = df[["feat1", group]].copy()
    tmp["split"] = "test"
    tmp.loc[tmp.index.isin(train_idx), "split"] = "basis"
    
    dhp_fold_fwd = DropHighPSIFeatures(split_col="split", bins=10, strategy="equal_frequency")
    dhp_fold_fwd.fit(tmp[["feat1", "split"]])
    psi_forward = dhp_fold_fwd.psi_values_["feat1"]
    
    dhp_fold_rev = DropHighPSIFeatures(split_col="split", bins=10, strategy="equal_frequency", switch=True)
    dhp_fold_rev.fit(tmp[["feat1", "split"]])
    psi_reverse = dhp_fold_rev.psi_values_["feat1"]
    
    rows.append({"Fold": fold,
                 "PSI_forward": psi_forward,
                 "PSI_reverse": psi_reverse})

df_folds = pd.DataFrame(rows).sort_values("Fold").reset_index(drop=True)


# ---------- Plot: PSI per fold (forward vs reverse) ----------
plt.figure()
x = np.arange(len(df_folds))
width = 0.35
plt.bar(x - width/2, df_folds["PSI_forward"], width, label="Forward (refer→test)")
plt.bar(x + width/2, df_folds["PSI_reverse"], width, label="Reverse (test→refer)")
plt.xticks(x, [f"Fold {i}" for i in df_folds["Fold"]])
plt.ylabel("PSI")
plt.title("PSI per Fold (Forward vs Reverse) — GroupKFold on YYYYMM")
plt.legend()
plt.tight_layout()
plt.show()

# ---------- Also show pure PSI as reference in text ----------
print(f"Pure PSI 2019→2020 (forward): {psi_pure_forward:.4f}")
print(f"Pure PSI 2020→2019 (reverse): {psi_pure_reverse:.4f}")


# from scipy.stats import ks_2samp
# ref =df2019_uneven['feat1']
# act = df2020_uneven['feat1']
# # Compute KS statistic and p-value
# ks_stat, ks_pval = ks_2samp(ref, act)

# #tools.display_dataframe_to_user("PSI and KS summary", summary_with_ks)

# # Plot the empirical CDFs to visualize the KS statistic
# def ecdf(data):
#     """Compute ECDF for a 1D array of measurements."""
#     x = np.sort(data)
#     y = np.arange(1, len(x)+1) / len(x)
#     return x, y

# x_ref, y_ref = ecdf(ref)
# x_act, y_act = ecdf(act)

# plt.figure(figsize=(8,6))
# plt.plot(x_ref, y_ref, label="Reference ECDF")
# plt.plot(x_act, y_act, label="Actual ECDF")
# plt.title(f"Empirical CDFs (KS statistic = {ks_stat:.4f}, p={ks_pval:.4e})")
# plt.xlabel("Value")
# plt.ylabel("ECDF")
# plt.legend()
# plt.grid(True)
# plt.tight_layout()
# plt.show()


import pandas as pd

def filter_independent_variable(df, app_col, bad_flag):
    """
    Filter applications per application_id by prioritizing 'bad' applications
    and applying 180-day spacing rule.

    Parameters:
        df (pd.DataFrame): Input DataFrame with columns [app_col, 'app_start_dt', bad_flag].
        app_col (str): Column name representing application id (or customer id).
        bad_flag (str): Column name that indicates bad cases (1 = bad, 0 = good).

    Returns:
        pd.DataFrame: Filtered DataFrame with selected applications.
    """

    # Ensure app_start_dt is datetime
    df["app_start_dt"] = pd.to_datetime(df["app_start_dt"])

    def process_group(k2):
        """Apply filtering logic to each group (per application_id)."""
        k2 = k2.drop_duplicates()

        # If only one row in group, return it
        if len(k2) == 1:
            return k2

        # Sort applications by start date (latest first)
        k2 = k2.sort_values("app_start_dt", ascending=False).reset_index(drop=True)

        filter_keep = set()       # Dates to keep
        filter_discard = set()    # Dates to discard

        # Start with the latest application
        track_dt = k2.iloc[0].app_start_dt
        track_bad = k2.iloc[0][bad_flag]

        # Loop through applications
        for i in range(1, len(k2)):
            date_diff = (track_dt - k2.iloc[i].app_start_dt).days

            if date_diff > 180:
                # Applications far apart → keep both
                filter_keep.add(track_dt)
                filter_keep.add(k2.iloc[i].app_start_dt)
                track_dt = k2.iloc[i].app_start_dt
                track_bad = k2.iloc[i][bad_flag]

            else:
                # Case 1: both bad
                if (track_bad == 1) and (k2.iloc[i][bad_flag] == 1):
                    filter_keep.add(track_dt)
                    track_dt = k2.iloc[i].app_start_dt
                    track_bad = 1

                # Case 2: current is bad → prefer it
                elif k2.iloc[i][bad_flag] == 1:
                    filter_keep.add(k2.iloc[i].app_start_dt)
                    track_dt = k2.iloc[i].app_start_dt
                    track_bad = 1

                    # If previous was within 180 days, discard it
                    if (k2.iloc[i-1].app_start_dt - k2.iloc[i].app_start_dt).days < 180:
                        filter_discard.add(k2.iloc[i-1].app_start_dt)

                # Case 3: otherwise → keep tracked one
                else:
                    filter_keep.add(track_dt)

        # Always add the last tracked date
        filter_keep.add(track_dt)

        # Final dates = keep - discard
        final_dates = list(filter_keep - filter_discard)

        return k2[k2.app_start_dt.isin(final_dates)]

    # Apply filtering per application_id group
    result = df.groupby(app_col, group_keys=False).apply(process_group)

    return result




