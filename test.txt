import numpy as np
import lightgbm as lgb
import optuna

from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

X, y = make_classification(
    n_samples=15000,
    n_features=20,
    n_informative=8,
    n_redundant=5,
    weights=[0.9, 0.1],   # 10% positives
    flip_y=0.01,
    random_state=42
)

dtrain = lgb.Dataset(X, label=y)

def stable_sigmoid(x):
    x = np.asarray(x)
    out = np.empty_like(x, dtype=np.float64)

    pos = x >= 0
    neg = ~pos

    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))
    exp_x = np.exp(x[neg])
    out[neg] = exp_x / (1.0 + exp_x)

    return out


class FocalLossObjective:
    def __init__(self, alpha, gamma):
        self.alpha = alpha
        self.gamma = gamma

    def __call__(self, preds, train_data):
        y = train_data.get_label()

        p = stable_sigmoid(preds)
        p = np.clip(p, 1e-7, 1 - 1e-7)

        grad = (
            self.alpha * y * (1 - p) ** self.gamma *
            (self.gamma * p * np.log(p) + p - 1)
            +
            (1 - self.alpha) * (1 - y) * p ** self.gamma *
            (self.gamma * (1 - p) * np.log(1 - p) + p)
        )

        hess = (
            self.alpha * y * (1 - p) ** self.gamma *
            ((self.gamma * (1 - p) - 1) * p +
             self.gamma * p * (1 - p) * np.log(p))
            +
            (1 - self.alpha) * (1 - y) * p ** self.gamma *
            ((self.gamma * p - 1) * (1 - p) +
             self.gamma * p * (1 - p) * np.log(1 - p))
        )

        return grad, np.maximum(hess, 1e-6)

    
from sklearn.metrics import roc_auc_score

def lgb_auc_eval(preds, train_data):
    y_true = train_data.get_label()
    preds_prob = stable_sigmoid(preds)
    auc = roc_auc_score(y_true, preds_prob)
    return "auc", auc, True


    
def run_lgb_cv(params, dtrain, X, y):

    cv_results = lgb.cv(
        params=params,
        train_set=dtrain,
        num_boost_round=2000,
        folds=StratifiedKFold(
            n_splits=5, shuffle=True, random_state=42
        ),
        feval=lgb_auc_eval,                    # âœ… REQUIRED
        callbacks=[lgb.early_stopping(100)],  # âœ… NOW VALID
        return_cvbooster=True,
        seed=42
    )

    # ---- Out-of-fold raw predictions ----
    cv_booster = cv_results["cvbooster"]

    raw_oof_preds = np.mean(
        [booster.predict(X) for booster in cv_booster.boosters],
        axis=0
    )

    oof_preds = stable_sigmoid(raw_oof_preds)
    auc = roc_auc_score(y, oof_preds)

    return auc

def objective(trial):

    alpha = trial.suggest_float("alpha", 0.1, 0.5)
    gamma = trial.suggest_float("gamma", 1.0, 3.0)

    params = {
    "objective": FocalLossObjective(alpha, gamma),
    "boosting_type": "gbdt",
    "feature_pre_filter": False,  # ðŸ”¥ REQUIRED
    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1, log=True),
    "num_leaves": trial.suggest_int("num_leaves", 16, 128),
    "max_depth": trial.suggest_int("max_depth", 3, 10),
    "min_child_samples": trial.suggest_int("min_child_samples", 50, 300),
    "subsample": trial.suggest_float("subsample", 0.7, 1.0),
    "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
    "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 10.0, log=True),
    "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 10.0, log=True),
    "verbose": -1,
    }


    return run_lgb_cv(params, dtrain, X, y)

sampler = optuna.samplers.TPESampler(
    seed=42,
    n_startup_trials=10,      # random exploration first
    n_ei_candidates=24,       # better candidate selection
    multivariate=True,        # models param interactions
    group=True                # good for correlated params
)

study = optuna.create_study(
    direction="maximize",
    sampler=sampler
)

study.optimize(objective, n_trials=20)

print("Best AUC:", study.best_value)
print("Best parameters:")
for k, v in study.best_params.items():
    print(f"  {k}: {v}")

