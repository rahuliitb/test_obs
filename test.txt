# =========================
# Imports
# =========================
import numpy as np
import pandas as pd
import optuna

from lightgbm import LGBMClassifier, early_stopping
from sklearn.model_selection import GroupKFold
from sklearn.metrics import roc_auc_score
from sklearn.datasets import make_classification


# =========================
# Optuna + LightGBM (sklearn API) + GroupKFold + Pruning
# =========================
def find_best_lgbm_params_sklearn(
    X_train,
    y_train,
    groups,
    n_trials: int = 30,
    n_splits: int = 3,
    random_state: int = 42,
):
    """
    LightGBM 4.x compatible:
    - sklearn API (LGBMClassifier)
    - GroupKFold CV
    - Optuna pruning
    - Early stopping via callbacks
    - Fully reproducible
    """

    def objective(trial):

        params = {
            # Core
            "boosting_type": "gbdt",
            "objective": "binary",

            # Capacity
            "learning_rate": trial.suggest_float("learning_rate", 0.03, 0.1),
            "num_leaves": trial.suggest_int("num_leaves", 16, 64),
            "max_depth": trial.suggest_int("max_depth", 4, 8),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 50, 300),

            # Regularization
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),

            # Subsampling
            "feature_fraction": trial.suggest_float("feature_fraction", 0.6, 0.9),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.6, 0.9),
            "bagging_freq": 1,

            # Training control
            "n_estimators": 1000,  # rely on early stopping

            # Reproducibility & stability
            "random_state": random_state,
            "n_jobs": 1,
            "deterministic": True,
            "force_row_wise": True,
            "feature_pre_filter": False,
            "verbosity": -1,
        }

        cv = GroupKFold(n_splits=n_splits)
        aucs = []

        for fold_idx, (train_idx, val_idx) in enumerate(
            cv.split(X_train, y_train, groups)
        ):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

            model = LGBMClassifier(**params)

            model.fit(
                X_tr,
                y_tr,
                eval_set=[(X_val, y_val)],
                eval_metric="auc",
                callbacks=[early_stopping(stopping_rounds=50, verbose=False)],
            )

            preds = model.predict_proba(
                X_val, num_iteration=model.best_iteration_
            )[:, 1]

            fold_auc = roc_auc_score(y_val, preds)
            aucs.append(fold_auc)

            # ---- Optuna pruning (fold-level, CV-safe) ----
            trial.report(np.mean(aucs), step=fold_idx)
            if trial.should_prune():
                raise optuna.TrialPruned()

        return float(np.mean(aucs))

    sampler = optuna.samplers.TPESampler(seed=random_state)
    pruner = optuna.pruners.MedianPruner(
        n_startup_trials=5,
        n_warmup_steps=1,
        interval_steps=1,
    )

    study = optuna.create_study(
        direction="maximize",
        sampler=sampler,
        pruner=pruner,
    )

    study.optimize(objective, n_trials=n_trials)

    best_params = {
        **study.best_params,
        "boosting_type": "gbdt",
        "objective": "binary",
        "bagging_freq": 1,
        "feature_pre_filter": False,
        "deterministic": True,
        "force_row_wise": True,
        "verbosity": -1,
        "random_state": random_state,
        "n_jobs": 1,
    }

    best_mean_auc = study.best_value

    return best_params, best_mean_auc, study


# =========================
# Small runnable test case
# =========================

# Synthetic binary classification data
X, y = make_classification(
    n_samples=1200,
    n_features=20,
    n_informative=8,
    n_redundant=6,
    n_clusters_per_class=2,
    weights=[0.7, 0.3],
    random_state=42,
)

# Convert to pandas (important for .iloc usage)
X = pd.DataFrame(X, columns=[f"f{i}" for i in range(X.shape[1])])
y = pd.Series(y)

# Create group labels (e.g., customer_id)
groups = np.repeat(np.arange(100), repeats=12)

# =========================
# Run Optuna
# =========================
best_params, best_mean_auc, study = find_best_lgbm_params_sklearn(
    X_train=X,
    y_train=y,
    groups=groups,
    n_trials=20,   # small for demo
    n_splits=3,
)

print("Best mean CV AUC:", round(best_mean_auc, 4))
print("\nBest parameters:")
for k, v in best_params.items():
    print(f"{k}: {v}")
