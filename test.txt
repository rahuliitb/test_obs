# Model Evaluation

This section describes the framework used to evaluate model performance. The evaluation focuses on discriminatory power, calibration quality, stability over time, and consistency across portfolio segments.

Model performance is assessed using a time-aware data split to evaluate robustness, generalization, and stability when applied to future data.

- **In-Time (IT):** Dataset used for model development.  
- **Out-of-Sample (OOS):** Estimated using cross-validation within the In-Time dataset.  
- **Out-of-Time (OOT):** Separate dataset from a later time period used to assess forward-looking stability.

---

## 1. Sample Composition

The table below summarizes the distribution of good and bad observations across the main evaluation datasets.

| Dataset | Total Observations | Good Count | Bad Count | Bad Rate (%) |
|----------|------------------|------------|-----------|--------------|
| In-Time (IT) | 50,000 | 47,000 | 3,000 | 6.00 |
| Out-of-Time (OOT) | 20,000 | 18,600 | 1,400 | 7.00 |

> *Note: Counts shown above are illustrative placeholders and should be replaced with final portfolio statistics.*

---

## 2. Discriminatory Power – AUC

The primary metric used to assess discriminatory power is the **Area Under the ROC Curve (AUC)**.

AUC measures the model’s ability to correctly rank defaulted observations higher than non-defaulted observations.

### 2.1 AUC Comparison Across Datasets

| Dataset | AUC (%) | Confidence Interval | Notes |
|----------|----------|--------------------|-------|
| In-Time (IT) | 82.23 | — | Development sample |
| OOS (Cross-Validated) | 75.11 | ± 2.72 | Cross-validated within IT |
| Out-of-Time (OOT) | 74.50 | [70.55, 78.31] | Future period validation sample |

The difference between IT and cross-validated OOS AUC is 7.12 percentage points. The difference between OOS and OOT AUC is 0.61 percentage points.

The alignment between OOS and OOT performance indicates that the model maintains stable discriminatory power when applied to future data. While In-Time performance is higher than cross-validated performance, this pattern is consistent with expected in-sample optimism. The performance comparison does not indicate material overfitting.

---

## 3. AUC Stability Over Time (In-Time)

To assess performance stability, AUC was evaluated on a quarterly basis within the In-Time dataset.

Observed AUC values range approximately between 0.68 and 0.82 across the evaluation period. Short-term fluctuations are visible; however, no persistent downward trend is observed across quarters.

Quarterly AUC estimates may vary due to differences in sample composition across periods. The absence of sustained deterioration suggests that the model’s discriminatory power remains stable over time within the development population.

---

## 4. Calibration Analysis

Calibration assesses how well predicted probabilities align with observed default rates. While discriminatory power measures ranking ability, calibration evaluates whether predicted risk levels are numerically reliable.

### 4.1 Calibration Curve

Calibration performance was evaluated using a **calibration curve** generated via the `calibration_curve` function from scikit-learn.

The calibration curve plots:

- **Fraction of positives (observed default rate)** on the y-axis  
- **Mean predicted probability** on the x-axis  

Predictions are grouped into probability bins, and for each bin the average predicted probability and corresponding observed default rate are calculated. A perfectly calibrated model lies along the 45-degree diagonal.

Calibration curves were evaluated for both In-Time (IT) and Out-of-Time (OOT) datasets.

Both IT and OOT curves closely follow the diagonal reference line, indicating strong alignment between predicted probabilities and observed default rates. The majority of predicted probabilities are concentrated in the lower probability range, consistent with the overall portfolio default rate. Minor deviations are observed in certain probability bins; however, these deviations are limited in magnitude and do not indicate a systematic pattern of overestimation or underestimation. The OOT curve closely mirrors the IT curve, confirming stable calibration under temporal validation.

---

### 4.2 Expected Calibration Error (ECE)

Calibration quality was also quantified using **Expected Calibration Error (ECE)**, which measures the average absolute difference between predicted probabilities and observed default rates across bins.

| Dataset | Expected Calibration Error (ECE) |
|----------|--------------------------------|
| In-Time (IT) | 0.0093 |
| Out-of-Time (OOT) | 0.0107 |

The low ECE values for both IT and OOT indicate minimal average deviation between predicted and observed risk. The close alignment between IT and OOT ECE further supports calibration stability under temporal validation.

---

## 5. Overall Assessment

The model demonstrates:

- Strong discriminatory power  
- Stable performance under cross-validation  
- Consistent Out-of-Time performance  
- Stable quarterly discriminatory power  
- Reliable and well-aligned probability calibration  

Based on the combined discrimination, stability, and calibration analyses, the model exhibits robust generalization characteristics and is suitable for production deployment.
