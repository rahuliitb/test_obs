# pip install lightgbm scikit-learn joblib pandas numpy scipy
import numpy as np
import pandas as pd
from typing import Optional, List, Dict
from joblib import Parallel, delayed
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score
from lightgbm import LGBMClassifier
from scipy.stats import randint as sp_randint, uniform as sp_uniform, loguniform as sp_loguniform, rv_discrete

# ---- Param distributions as scipy.stats distributions ----
#_max_depth_vals = np.array([-1, 3, 4, 5, 6, 7, 8])
_max_depth_probs = np.full(len(_max_depth_vals), 1/len(_max_depth_vals))
MAX_DEPTH_DIST = rv_discrete(name="max_depth", values=(_max_depth_vals, _max_depth_probs))

PARAM_DIST = {
    "clf__num_leaves": sp_randint(3, 5),
    "clf__min_child_samples": sp_randint(10, 200),
   # "clf__max_depth": MAX_DEPTH_DIST,
    "clf__learning_rate": sp_loguniform(1e-2, 2e-1),
    "clf__subsample": sp_uniform(loc=0.7, scale=0.3),
    "clf__colsample_bytree": sp_uniform(loc=0.7, scale=0.3),
    "clf__reg_lambda": sp_loguniform(1e-4, 10.0),
}

def _make_imputer(impute_strategy: Optional[str]) -> Optional[SimpleImputer]:
    if impute_strategy is None:
        return None
    if isinstance(impute_strategy, (int, float)) and impute_strategy == 0:
        return SimpleImputer(strategy="constant", fill_value=0)
    if str(impute_strategy).lower() in {"zero", "constant_zero"}:
        return SimpleImputer(strategy="constant", fill_value=0)
    return SimpleImputer(strategy=str(impute_strategy))

def _run_one_feature_randomcv(
    X_col: pd.Series,
    y_arr: np.ndarray,
    feature: str,
    n_iter: int,
    cv,
    seed: int,
    impute_strategy: Optional[str],
) -> Dict:
    steps = []
    imputer = _make_imputer(impute_strategy)
    if imputer is not None:
        steps.append(("imputer", imputer))

    clf = LGBMClassifier(
        objective="binary",
        metric="auc",
        n_estimators=int(2000),
        random_state=int(seed),
        n_jobs=1,
    )
    pipe = Pipeline(steps + [("clf", clf)])

    # Random search
    search = RandomizedSearchCV(
        estimator=pipe,
        param_distributions=PARAM_DIST,
        n_iter=int(n_iter),
        scoring="roc_auc",
        cv=cv,
        random_state=int(seed),
        n_jobs=1,
        refit=True,
        verbose=0,
    )
    X_feat = X_col.to_numpy().reshape(-1, 1)
    search.fit(X_feat, y_arr)

    best_cv_auc = float(search.best_score_)
    best_model = search.best_estimator_
    train_proba = best_model.predict_proba(X_feat)[:, 1]
    train_auc = float(roc_auc_score(y_arr, train_proba))

    # ---- NEW: average fold train/valid AUC using best params ----
    fold_train_aucs, fold_valid_aucs = [], []
    for tr_idx, va_idx in cv.split(X_feat, y_arr):
        X_tr, X_va = X_feat[tr_idx], X_feat[va_idx]
        y_tr, y_va = y_arr[tr_idx], y_arr[va_idx]

        m = Pipeline(steps + [("clf", LGBMClassifier(
            **{k.replace("clf__", ""): v for k, v in search.best_params_.items()},
            objective="binary",
            metric="auc",
            n_estimators=int(2000),
            random_state=int(seed),
            n_jobs=1,
        ))])
        m.fit(X_tr, y_tr)
        proba_tr = m.predict_proba(X_tr)[:, 1]
        proba_va = m.predict_proba(X_va)[:, 1]
        fold_train_aucs.append(roc_auc_score(y_tr, proba_tr))
        fold_valid_aucs.append(roc_auc_score(y_va, proba_va))

    avg_train_auc = float(np.mean(fold_train_aucs))
    avg_valid_auc = float(np.mean(fold_valid_aucs))

    prefix = "with_impute" if imputer is not None else "no_impute"
    return {
        "feature": feature,
        f"AUC_valid_{prefix}": best_cv_auc,
        f"AUC_train_{prefix}": train_auc,
        f"Avg_fold_AUC_valid_{prefix}": avg_valid_auc,
        f"Avg_fold_AUC_train_{prefix}": avg_train_auc,
        f"best_params_{prefix}": search.best_params_,
    }

def univariate_lgbm_importance_randomcv_parallel(
    df: pd.DataFrame,
    y: pd.Series,
    features: Optional[List[str]] = None,
    n_splits: int = 5,
    n_iter: int = 20,
    seed: int = 42,
    n_jobs: int = -1,
    compare_impute: bool = True,
    impute_strategy: str = "median",
) -> pd.DataFrame:
    if features is None:
        features = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]

    X = df[features]
    y_arr = y.astype(int).to_numpy()
    cv = StratifiedKFold(n_splits=int(n_splits), shuffle=True, random_state=int(seed))

    res_noimp = Parallel(n_jobs=n_jobs, prefer="processes")(
        delayed(_run_one_feature_randomcv)(
            X[col], y_arr, col, n_iter, cv, seed, impute_strategy=None
        )
        for col in features
    )
    df_noimp = pd.DataFrame(res_noimp)

    if compare_impute:
        res_imp = Parallel(n_jobs=n_jobs, prefer="processes")(
            delayed(_run_one_feature_randomcv)(
                X[col], y_arr, col, n_iter, cv, seed, impute_strategy=impute_strategy
            )
            for col in features
        )
        df_imp = pd.DataFrame(res_imp)
        out = df_noimp.merge(df_imp, on="feature", how="left")
        sort_col = "AUC_valid_with_impute" if "AUC_valid_with_impute" in out else "AUC_valid_no_impute"
    else:
        out = df_noimp
        sort_col = "AUC_valid_no_impute"

    return out.sort_values(sort_col, ascending=False).reset_index(drop=True)

long_df = (wide_counts
  .select(
    F.array(
      F.col("after_00_universe"), F.col("after_01_has_acct"), F.col("after_02_cust_type"),
      F.col("after_03_mom_segm"), F.col("after_04_exc_rating"), F.col("after_05_legal_form"),
      F.col("after_06_limit_amt"), F.col("after_07_model_prefix"), F.col("after_08_dpd30_ok"),
      F.col("after_09_not_kos_imputed"), F.col("after_10_max_days"), F.col("after_11_l00_after_ok"),
      F.col("after_12_bad_elig_ok"), F.col("after_13_main_grid_ok"), F.col("after_14_bad_flag_notnull"),
      F.col("after_15_ctry_ok")
    ).alias("remaining_arr"),
    F.array(
      F.col("bads_00_universe"), F.col("bads_01_has_acct"), F.col("bads_02_cust_type"),
      F.col("bads_03_mom_segm"), F.col("bads_04_exc_rating"), F.col("bads_05_legal_form"),
      F.col("bads_06_limit_amt"), F.col("bads_07_model_prefix"), F.col("bads_08_dpd30_ok"),
      F.col("bads_09_not_kos_imputed"), F.col("bads_10_max_days"), F.col("bads_11_l00_after_ok"),
      F.col("bads_12_bad_elig_ok"), F.col("bads_13_main_grid_ok"), F.col("bads_14_bad_flag_notnull"),
      F.col("bads_15_ctry_ok")
    ).alias("bads_arr")
  )
  # 1) Create step/remaining + keep bads_arr
  .select(
    F.posexplode("remaining_arr").alias("step", "remaining"),
    F.col("bads_arr")
  )
  # 2) Now you can reference step
  .withColumn("bads_remaining", F.col("bads_arr")[F.col("step")])
  .select("step", "remaining", "bads_remaining")
)

from pyspark.sql import functions as F

lag_months = 12

# Step 1: Truncate dates
joined_df_ana = (
    joined_df
    .withColumn("app_month", F.trunc("APP_START_DT", "MM"))
    .withColumn("trx_month", F.trunc("TJD", "MM"))
)

# Step 2: Minimum transaction month per application
df_min_trx = (
    joined_df_ana
    .groupBy("PROD_KEY", "GRIDID", "app_month")
    .agg(F.min("trx_month").alias("min_trx_date"))
)

# Step 3: Flag whether borrower has â‰¥ lag_months history
df_result = df_min_trx.withColumn(
    "has_lag_month_history",
    F.col("min_trx_date").isNotNull() &
    (F.col("app_month") >= F.add_months(F.col("min_trx_date"), lag_months))
)

# Step 4: Join back (KEEP ALL applications, no filter here)
df_with_flag = joined_df_ana.join(
    F.broadcast(df_result), on=["PROD_KEY", "GRIDID", "app_month"], how="left"
)

# Step 5: Filter transactions in the last lag_months before app_month
df_last_12_months = df_with_flag.filter(
    (F.col("TJD") >= F.add_months(F.col("app_month"), -lag_months)) &
    (F.col("TJD") <= F.col("app_month"))
)

# Step 6: Monthly transaction counts
df_month_counts = (
    df_last_12_months
    .groupBy("PROD_KEY", "GRIDID", "app_month", "trx_month", "has_lag_month_history")
    .agg(
        F.count("*").alias("monthly_trx_count"),
        F.sum(F.when(F.col("ITC1") == 1, 1).otherwise(0)).alias("m_credit_trx_count")
    )
)

# Step 7: Aggregate at application level
df_final = (
    df_month_counts
    .groupBy("PROD_KEY", "GRIDID", "app_month", "has_lag_month_history")
    .agg(
        F.countDistinct("trx_month").alias("active_month"),
        F.min("monthly_trx_count").alias("min_trx_count_nonzero"),
        F.sum("m_credit_trx_count").alias("total_credit_trx_count"),
        F.sum("monthly_trx_count").alias("total_trx_count")
    )
    .withColumn(
        "min_trx_count",
        F.when(F.col("active_month") == lag_months, F.col("min_trx_count_nonzero"))
         .otherwise(F.lit(0))
    )
    .drop("min_trx_count_nonzero")
)

# Final output
df_final.show(truncate=False)



